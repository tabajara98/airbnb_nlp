{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4f7f95-e2b2-4b29-8d9c-3b0faa9f51e4",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11d8a18f-b8dc-4662-8541-424d00128d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f304a4-5fc5-4425-a828-4350651bc53e",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe14cd-99fc-48df-9aab-864d75ed1d74",
   "metadata": {},
   "source": [
    "#### Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30a39045-f8d0-416f-8c34-4cec81160243",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_folder_path = '..\\\\data\\\\raw\\\\listings'\n",
    "\n",
    "df_listings = pd.DataFrame()\n",
    "for listing_file in os.listdir(listings_folder_path):\n",
    "    listing_file_path = os.path.join(listings_folder_path,listing_file) \n",
    "    df = pd.read_csv(listing_file_path,compression='gzip')\n",
    "    df_listings = pd.concat([df,df_listings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4a5f0a2-3643-4492-84ad-2c6007ab78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split name into actual name and summary\n",
    "df_listings['subtext'] = df_listings['name'].str.split(' · ').str[1:].apply(lambda x: ' · '.join(x))\n",
    "df_listings['subtext'] = df_listings['subtext'].str.replace('·','•')\n",
    "df_listings['name'] = df_listings['name'].str.split(' · ').str[0]\n",
    "\n",
    "df_listings.rename(columns={'listing_url':'link','picture_url':'photo','neighbourhood':'location'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3c8c2-e7f4-4a09-be4d-5f5293353135",
   "metadata": {},
   "source": [
    "#### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "282910ba-7118-4afb-b104-b3310b1b7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_folder_path = '..\\\\data\\\\raw\\\\reviews'\n",
    "\n",
    "df_reviews = pd.DataFrame()\n",
    "for review_file in os.listdir(reviews_folder_path):\n",
    "    review_file_path = os.path.join(reviews_folder_path,review_file) \n",
    "    df = pd.read_csv(review_file_path,compression='gzip')\n",
    "    df_reviews = pd.concat([df,df_reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3be579-669a-4028-94cc-a1ca167949aa",
   "metadata": {},
   "source": [
    "### Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a96f4243-1e87-4224-a7f1-25814ba27ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listings\n",
    "listings_id_column = 'id'\n",
    "listings_nlp_columns = [\n",
    "    'amenities',\n",
    "    'accommodates',\n",
    "    'name',\n",
    "    'subtext',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'location',\n",
    "    'neighbourhood_cleansed',\n",
    "    'description'\n",
    "]\n",
    "\n",
    "# Other columns to save\n",
    "cols_aux_final = ['id','name','subtext','description','link','photo','price','location']\n",
    "\n",
    "df_listings.loc[:,'corpus_text_host'] = ''\n",
    "for nlp_col in listings_nlp_columns:\n",
    "    df_listings.loc[:,'corpus_text_host'] += ' ' + df_listings.loc[:,nlp_col].fillna('').astype(str)+ '. '\n",
    "df_listings = df_listings[cols_aux_final+['corpus_text_host']]    \n",
    "\n",
    "# Reviews\n",
    "df_reviews_grouped_id = df_reviews.groupby(\n",
    "    by='listing_id',\n",
    "    as_index=False\n",
    ").agg(\n",
    "    {'comments': lambda review: ' '.join(review.fillna(''))}\n",
    ")\n",
    "\n",
    "# Final\n",
    "df = pd.merge(\n",
    "    left=df_listings,\n",
    "    right=df_reviews_grouped_id,\n",
    "    left_on='id',\n",
    "    right_on='listing_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df.rename(columns={'comments':'corpus_text_reviews'},inplace=True)\n",
    "df.drop(['listing_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6f478-6375-4327-be22-bfb541e34684",
   "metadata": {},
   "source": [
    "### Preprocess for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad83fb12-2a98-419a-b002-2d18d940a71f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to perform all cleaning steps\n",
    "def clean_text(text):\n",
    "        \n",
    "    # Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [porter_stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Set of English stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574c0b2-6638-46f9-a6f3-ef86679e4944",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d017744-e954-4e69-9590-f383e9139849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff71330-9fc6-4e6b-9831-cd5c035db8b8",
   "metadata": {},
   "source": [
    "#### Encode Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "734fe9cf-0c54-4b0d-a701-2c118b14084a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### INITIALIZING SBERT MODEL #####\n",
      "##### CACHED EMBEDDINGS PICKLE NOT FOUND #####\n",
      "##### ENCODING ALL CORPUS TEXTS #####\n",
      "> embeddings_host\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b332596a44e4e699196db6ada16dd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m corpus_name, corpus_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_host\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m],[corpus_texts_host, corpus_texts_reviews]):\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorpus_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 78\u001b[0m     corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     storage_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mcorpus_name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m corpus_text \n\u001b[0;32m     81\u001b[0m     storage_dict[corpus_name] \u001b[38;5;241m=\u001b[39m corpus_embeddings\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SBERT model name\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "\n",
    "# Initialize SBERT model\n",
    "print('##### INITIALIZING SBERT MODEL #####')\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Cached Embeddings Path (changes according to model)\n",
    "embedding_cache_path = f'cache\\\\cached-embeddings-{model_name}_weighted_clean.pkl'\n",
    "\n",
    "# Current corpus texts\n",
    "current_corpus_texts_host = df['corpus_text_host'].fillna('')\n",
    "current_corpus_texts_reviews = df['corpus_text_reviews'].fillna('')\n",
    "   \n",
    "# If cache pkl file path exists\n",
    "if os.path.exists(embedding_cache_path):\n",
    "    None\n",
    "#     print('##### CACHED EMBEDDINGS PICKLE FOUND #####')\n",
    "\n",
    "#     # Read cached embeddings\n",
    "#     with open(embedding_cache_path, \"rb\") as fIn:\n",
    "#         cache_data = pickle.load(fIn)\n",
    "    \n",
    "#     # Extract corpus text and embeddings from cache pkl\n",
    "#     cache_corpus_texts_host = cache_data['text_embeddings_host']\n",
    "#     cache_corpus_texts_reviews = cache_data['text_embeddings_reviews']\n",
    "#     cache_corpus_embeddings_host = cache_data['embeddings_host']\n",
    "#     cache_corpus_embeddings_reviews = cache_data['embeddings_reviews']\n",
    "    \n",
    "#     print('##### IDENTIFYING CORPUS TEXTS NOT IN CACHE #####')\n",
    "#     corpus_text_host_not_in_cache = []\n",
    "#     corpus_text_reviews_not_in_cache = []\n",
    "#     for text_host, text_reviews in zip(current_corpus_texts_host, current_corpus_texts_reviews):\n",
    "#         if text_reviews not in cache_corpus_texts_reviews:\n",
    "#             print('> TEXT NO. {:,.0f} ({:,.0%} OF TOTAL DATASET)'.format(i, len(corpus_text_host_not_in_cache)/len(current_corpus_texts_reviews)))\n",
    "#             corpus_text_host_not_in_cache.append(text_host)\n",
    "#             corpus_text_reviews_not_in_cache.append(text_reviews)\n",
    "    \n",
    "#     if corpus_text_reviews_not_in_cache != []:\n",
    "        \n",
    "#         # Apply the cleaning function to the 'corpus_text' column\n",
    "#         print('##### CLEANING NEW CORPUS TEXTS #####')\n",
    "#         corpus_text_reviews_not_in_cache = pd.Series(corpus_text_reviews_not_in_cache).apply(clean_text).to_list()\n",
    "#         corpus_text_host_not_in_cache = pd.Series(corpus_text_host_not_in_cache).apply(clean_text).to_list()\n",
    "\n",
    "#         # Encode ONLY the current corpus texts that aren't in cache into embeddings\n",
    "#         print('##### ENCODING IDENTIFIED CORPUS TEXTS #####')\n",
    "#         remaining_corpus_embeddings_reviews = model.encode(corpus_text_reviews_not_in_cache,show_progress_bar=True,convert_to_tensor=True)\n",
    "#         remaining_corpus_embeddings_host = model.encode(corpus_text_host_not_in_cache,show_progress_bar=True,convert_to_tensor=True)\n",
    "        \n",
    "#     else:\n",
    "#         print('> NO NEW CORPUS TEXTS')\n",
    "#         remaining_corpus_embeddings_reviews = torch.empty(0)\n",
    "#         remaining_corpus_embeddings_host = torch.empty(0)\n",
    "        \n",
    "#     # Joining corpus data into single objects for export later\n",
    "#     corpus_embeddings_host = torch.cat((cache_corpus_embeddings_host,remaining_corpus_embeddings_host), dim=0)\n",
    "#     corpus_embeddings_reviews = torch.cat((cache_corpus_embeddings_reviews,remaining_corpus_embeddings_reviews), dim=0)\n",
    "    \n",
    "#     corpus_texts_host = cache_corpus_texts_host + corpus_text_host_not_in_cache\n",
    "#     corpus_texts_reviews = cache_corpus_texts_reviews + corpus_text_reviews_not_in_cache\n",
    "    \n",
    "#     # Update & export complete text and embeddings as pkl for future executions\n",
    "#     print('##### EXPORTING  #####')\n",
    "#     with open(embedding_cache_path, \"wb\") as fOut:\n",
    "#         pickle.dump(storage_dict, fOut)\n",
    "    \n",
    "else:\n",
    "    print('##### CACHED EMBEDDINGS PICKLE NOT FOUND #####')\n",
    "    corpus_texts_host = current_corpus_texts_host\n",
    "    corpus_texts_reviews = current_corpus_texts_reviews\n",
    "\n",
    "    # Encode ALL the current corpus texts into embeddings\n",
    "    print('##### ENCODING ALL CORPUS TEXTS #####')\n",
    "    storage_dict = {}\n",
    "    for corpus_name, corpus_text in zip(['embeddings_host','embeddings_reviews'],[corpus_texts_host, corpus_texts_reviews]):\n",
    "        print(f'> {corpus_name}')\n",
    "        corpus_embeddings = model.encode(corpus_text,show_progress_bar=True,convert_to_tensor=True)\n",
    "        \n",
    "        storage_dict['text_'+corpus_name.split('_')[1]] = corpus_text \n",
    "        storage_dict[corpus_name] = corpus_embeddings\n",
    "        \n",
    "    for col in cols_aux_final:\n",
    "        storage_dict[col] = df[col].to_list()\n",
    "        \n",
    "# Update & export complete text and embeddings as pkl for future executions\n",
    "print('##### EXPORTING  #####')\n",
    "with open(embedding_cache_path, \"wb\") as fOut:\n",
    "    pickle.dump(storage_dict, fOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c95ea-5e0e-4d95-8d27-636e10ff21ee",
   "metadata": {},
   "source": [
    "#### Weight Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a419279-cf6e-47ff-8be9-1e08cd08170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight tensor\n",
    "weights = torch.tensor([0.5, 0.5])\n",
    "embeddings = ['embeddings_host','embeddings_reviews']\n",
    "corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "for i, corpus in enumerate(embeddings):\n",
    "    \n",
    "    # Weight the vectors with the specified weights\n",
    "    weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "    \n",
    "    \n",
    "    # Add the weighted embeddings to the corpus_embeddings\n",
    "    corpus_embeddings += weighted_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31ecc-a88c-4e96-95ef-a343b90eb6f6",
   "metadata": {},
   "source": [
    "#### Encode Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b05a61-ceba-4477-bb48-806345496e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the query\n",
    "query = \"Cozy cabin close to beach\"\n",
    "clean_query = pd.Series(query).apply(clean_text)\n",
    "query_embedding = model.encode(query,show_progress_bar=True,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aec154-05ce-4ce8-bd61-ff41ccbed3fc",
   "metadata": {},
   "source": [
    "#### Apply Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5ed85-b614-423a-bcb3-f10cde096805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "\n",
    "search_results = util.semantic_search(\n",
    "    query_embedding, corpus_embeddings, top_k=top_k\n",
    ")\n",
    "\n",
    "# Extract the indices of the most similar sentences\n",
    "similar_indices = search_results[0][0:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d99cb35-91c1-4307-a7b3-b6759ae5b6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>picture_url</th>\n",
       "      <th>price</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33100</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.665866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21813</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.662432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14317</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.661356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24282</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.655653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.654913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22119</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.649561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.649321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29443</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.647911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17401</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.647903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>35910806.0</td>\n",
       "      <td>Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths</td>\n",
       "      <td>Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The space&lt;/b&gt;&lt;br /&gt;Whites Landing is a secluded cove 3 miles east ...</td>\n",
       "      <td>https://www.airbnb.com/rooms/35910806</td>\n",
       "      <td>https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg</td>\n",
       "      <td>$152.00</td>\n",
       "      <td>Avalon, California, United States</td>\n",
       "      <td>0.646953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                                      name  \\\n",
       "33100  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "21813  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "14317  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "24282  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "3610   35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "22119  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "2465   35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "29443  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "17401  35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "1434   35910806.0  Nature lodge in Avalon · ★4.20 · 8 beds · 0 shared baths   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               description  \\\n",
       "33100  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "21813  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "14317  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "24282  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "3610   Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "22119  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "2465   Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "29443  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "17401  Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "1434   Our Safari tents provide you with the basics. You will have a large tent cabin that sleeps up to 8 ppl, 4 bunk beds w/mattresses and a nice deck in front. Your tent is just steps from the beach, bathrooms, and the camp store. Bring a fitted sheet, a sleeping bag,  and toiletries. At night when all is quiet, sit on your deck, gaze up at the stars, listen to the waves crashing on the beach. You are on island time!<br /><br /><b>The space</b><br />Whites Landing is a secluded cove 3 miles east ...   \n",
       "\n",
       "                                 listing_url  \\\n",
       "33100  https://www.airbnb.com/rooms/35910806   \n",
       "21813  https://www.airbnb.com/rooms/35910806   \n",
       "14317  https://www.airbnb.com/rooms/35910806   \n",
       "24282  https://www.airbnb.com/rooms/35910806   \n",
       "3610   https://www.airbnb.com/rooms/35910806   \n",
       "22119  https://www.airbnb.com/rooms/35910806   \n",
       "2465   https://www.airbnb.com/rooms/35910806   \n",
       "29443  https://www.airbnb.com/rooms/35910806   \n",
       "17401  https://www.airbnb.com/rooms/35910806   \n",
       "1434   https://www.airbnb.com/rooms/35910806   \n",
       "\n",
       "                                                                     picture_url  \\\n",
       "33100  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "21813  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "14317  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "24282  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "3610   https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "22119  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "2465   https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "29443  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "17401  https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "1434   https://a0.muscache.com/pictures/f9c0763e-9446-4d85-87b1-3a14cfab7cf3.jpg   \n",
       "\n",
       "         price                      neighbourhood     Score  \n",
       "33100  $152.00  Avalon, California, United States  0.665866  \n",
       "21813  $152.00  Avalon, California, United States  0.662432  \n",
       "14317  $152.00  Avalon, California, United States  0.661356  \n",
       "24282  $152.00  Avalon, California, United States  0.655653  \n",
       "3610   $152.00  Avalon, California, United States  0.654913  \n",
       "22119  $152.00  Avalon, California, United States  0.649561  \n",
       "2465   $152.00  Avalon, California, United States  0.649321  \n",
       "29443  $152.00  Avalon, California, United States  0.647911  \n",
       "17401  $152.00  Avalon, California, United States  0.647903  \n",
       "1434   $152.00  Avalon, California, United States  0.646953  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the actual sentences\n",
    "df_result = pd.DataFrame()\n",
    "\n",
    "for col in ['id', 'name', 'description', 'listing_url', 'picture_url', 'price', 'neighbourhood']:\n",
    "    for indice in [similar_indices[i]['corpus_id'] for i in range(len(similar_indices))]:\n",
    "        df_result.loc[indice,col] = storage_dict[col][i]\n",
    "    \n",
    "df_result['Score'] = [item['score'] for item in similar_indices]\n",
    "\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b8ae-ea7e-4ccc-acbf-c2af9c922479",
   "metadata": {},
   "source": [
    "### Test optimal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc4af379-5bb0-455d-9b9b-b8fd662ebb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac3f50a2-5d14-41d4-8cd1-56d2cea49d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.00, 1.00] | Max Score: 0.488\n",
      "Weights: [0.10, 0.90] | Max Score: 0.487\n",
      "Weights: [0.20, 0.80] | Max Score: 0.483\n",
      "Weights: [0.30, 0.70] | Max Score: 0.474\n",
      "Weights: [0.40, 0.60] | Max Score: 0.462\n",
      "Weights: [0.50, 0.50] | Max Score: 0.448\n",
      "Weights: [0.60, 0.40] | Max Score: 0.438\n",
      "Weights: [0.70, 0.30] | Max Score: 0.424\n",
      "Weights: [0.80, 0.20] | Max Score: 0.407\n",
      "Weights: [0.90, 0.10] | Max Score: 0.390\n"
     ]
    }
   ],
   "source": [
    "# Create a weight tensor\n",
    "\n",
    "for j in range(10):\n",
    "    \n",
    "    weights = torch.tensor([j/10, 1-(j/10)])\n",
    "    embeddings = ['embeddings_host','embeddings_reviews']\n",
    "    corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "    for i, corpus in enumerate(embeddings):\n",
    "\n",
    "        # Weight the vectors with the specified weights\n",
    "        weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "\n",
    "\n",
    "        # Add the weighted embeddings to the corpus_embeddings\n",
    "        corpus_embeddings += weighted_embeddings\n",
    "        \n",
    "    # Encode the query\n",
    "    query = \"Romantic for couple in mountains\"\n",
    "    clean_query = pd.Series(query).apply(clean_text)\n",
    "    query_embedding = model.encode(query,show_progress_bar=False,convert_to_tensor=True)\n",
    "    search_results = util.semantic_search(\n",
    "        query_embedding, corpus_embeddings, top_k=1\n",
    "    )\n",
    "\n",
    "    # Extract the indices of the most similar sentences\n",
    "    score = search_results[0][0]['score']\n",
    "    \n",
    "    print('Weights: [{:,.2f}, {:,.2f}]'.format(j/10, 1-(j/10)), '| Max Score: {:,.3f}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
