{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4f7f95-e2b2-4b29-8d9c-3b0faa9f51e4",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d8a18f-b8dc-4662-8541-424d00128d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f304a4-5fc5-4425-a828-4350651bc53e",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe14cd-99fc-48df-9aab-864d75ed1d74",
   "metadata": {},
   "source": [
    "#### Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a39045-f8d0-416f-8c34-4cec81160243",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_folder_path = '..\\\\data\\\\raw\\\\listings'\n",
    "\n",
    "df_listings = pd.DataFrame()\n",
    "for listing_file in os.listdir(listings_folder_path):\n",
    "    listing_file_path = os.path.join(listings_folder_path,listing_file) \n",
    "    df = pd.read_csv(listing_file_path,compression='gzip')\n",
    "    df_listings = pd.concat([df,df_listings])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3c8c2-e7f4-4a09-be4d-5f5293353135",
   "metadata": {},
   "source": [
    "#### Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282910ba-7118-4afb-b104-b3310b1b7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_folder_path = '..\\\\data\\\\raw\\\\reviews'\n",
    "\n",
    "df_reviews = pd.DataFrame()\n",
    "for review_file in os.listdir(reviews_folder_path):\n",
    "    review_file_path = os.path.join(reviews_folder_path,review_file) \n",
    "    df = pd.read_csv(review_file_path,compression='gzip')\n",
    "    df_reviews = pd.concat([df,df_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12c3b6f-bac5-481d-a127-713664c7291b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8941071</td>\n",
       "      <td>68391055</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>10164333</td>\n",
       "      <td>Smruti</td>\n",
       "      <td>Danielle was a great host, she was extremely responsive to any questions I had, was flexible when I had to extend my stay and was very kind.  The apartment is exactly how it looks in the pictures - very cute, clean and comfortable.  The location is amazing on a quiet block but very close to restaurants and a Whole Foods.  I stayed here for 2 months when I started my job and it really felt like home. Thanks Danielle!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8941071</td>\n",
       "      <td>153719836</td>\n",
       "      <td>2017-05-21</td>\n",
       "      <td>97944097</td>\n",
       "      <td>Rob</td>\n",
       "      <td>The apartment was great for us to spend the weekend in WeHo, very clean and quiet at night but just a street away from good restaurants, cafes and bars. For us the only thing missing was an iron for our clothes. The underground parking is good but our BMW X5 was as big as the space would fit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8941071</td>\n",
       "      <td>147589354</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>4123723</td>\n",
       "      <td>Widya</td>\n",
       "      <td>Danielle is a great host, very concerned  with her guests well-being, easy to reach and fast to reply. The apartment is perfectly located, walking access to supplies, shopping and hip places, or just for jogging the neighborhood:-) Can't be better! The apartment is spacious and bright, very comfortable. Few things are missing, like a toaster, and it is a bit darkly lit at night. But it's definitely great value for money.&lt;br/&gt;Thank you, Danielle!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8941071</td>\n",
       "      <td>145742425</td>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>1459499</td>\n",
       "      <td>Darian</td>\n",
       "      <td>Great location and spacious. Danielle's place was a home away from home. She was also very flexible with check in/out times, which I really appreciated.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8941071</td>\n",
       "      <td>144400833</td>\n",
       "      <td>2017-04-15</td>\n",
       "      <td>98494277</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>Danielle's place was as expected, really good location, calm and cosy.&lt;br/&gt;Danielle was very helpful to make it a stress free experience !&lt;br/&gt;Would recommend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id         id        date  reviewer_id reviewer_name  \\\n",
       "0     8941071   68391055  2016-04-04     10164333        Smruti   \n",
       "1     8941071  153719836  2017-05-21     97944097           Rob   \n",
       "2     8941071  147589354  2017-04-27      4123723         Widya   \n",
       "3     8941071  145742425  2017-04-19      1459499        Darian   \n",
       "4     8941071  144400833  2017-04-15     98494277       Charlie   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                            comments  \n",
       "0                                Danielle was a great host, she was extremely responsive to any questions I had, was flexible when I had to extend my stay and was very kind.  The apartment is exactly how it looks in the pictures - very cute, clean and comfortable.  The location is amazing on a quiet block but very close to restaurants and a Whole Foods.  I stayed here for 2 months when I started my job and it really felt like home. Thanks Danielle!  \n",
       "1                                                                                                                                                             The apartment was great for us to spend the weekend in WeHo, very clean and quiet at night but just a street away from good restaurants, cafes and bars. For us the only thing missing was an iron for our clothes. The underground parking is good but our BMW X5 was as big as the space would fit.   \n",
       "2  Danielle is a great host, very concerned  with her guests well-being, easy to reach and fast to reply. The apartment is perfectly located, walking access to supplies, shopping and hip places, or just for jogging the neighborhood:-) Can't be better! The apartment is spacious and bright, very comfortable. Few things are missing, like a toaster, and it is a bit darkly lit at night. But it's definitely great value for money.<br/>Thank you, Danielle!  \n",
       "3                                                                                                                                                                                                                                                                                                           Great location and spacious. Danielle's place was a home away from home. She was also very flexible with check in/out times, which I really appreciated.  \n",
       "4                                                                                                                                                                                                                                                                                                    Danielle's place was as expected, really good location, calm and cosy.<br/>Danielle was very helpful to make it a stress free experience !<br/>Would recommend   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6f478-6375-4327-be22-bfb541e34684",
   "metadata": {},
   "source": [
    "### Preprocess for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad83fb12-2a98-419a-b002-2d18d940a71f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to perform all cleaning steps\n",
    "def clean_text(text):\n",
    "        \n",
    "    # Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [porter_stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Set of English stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574c0b2-6638-46f9-a6f3-ef86679e4944",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5d017744-e954-4e69-9590-f383e9139849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_reviews.loc[:1000,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff71330-9fc6-4e6b-9831-cd5c035db8b8",
   "metadata": {},
   "source": [
    "#### Encode Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bc8a365-b76c-4083-a480-e1f98c0aa30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaned = pd.Series(df['comments'].fillna('')).apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1af60616-fff2-4457-9702-0fdd3079fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14a84d0d-1b6f-4578-8f99-df1041b413e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = corpus_df[corpus_df['comments'].apply(lambda x: bool(x))]\n",
    "\n",
    "def join_non_empty(lst):\n",
    "    non_empty_lst = [item for item in lst if item]\n",
    "    return ', '.join(non_empty_lst)\n",
    "\n",
    "corpus_df['corpus'] = corpus_df['comments'].apply(join_non_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "169225d0-0086-4b8e-8ffa-0899ec9da814",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = corpus_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "734fe9cf-0c54-4b0d-a701-2c118b14084a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### INITIALIZING SBERT MODEL #####\n",
      "##### CACHED EMBEDDINGS PICKLE NOT FOUND #####\n",
      "##### ENCODING ALL CORPUS TEXTS #####\n",
      "> embeddings_reviews\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e133eb1539df403cbe56d66b62e0ddc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### EXPORTING  #####\n"
     ]
    }
   ],
   "source": [
    "# SBERT model name\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "\n",
    "# Initialize SBERT model\n",
    "print('##### INITIALIZING SBERT MODEL #####')\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Cached Embeddings Path (changes according to model)\n",
    "embedding_cache_path = f'cache\\\\cached-embeddings-{model_name}_mean_average_clean.pkl'\n",
    "\n",
    "# Current corpus texts\n",
    "current_corpus_texts_reviews = corpus_df['corpus']\n",
    "   \n",
    "# If cache pkl file path exists\n",
    "if os.path.exists(embedding_cache_path):\n",
    "    None\n",
    "#     print('##### CACHED EMBEDDINGS PICKLE FOUND #####')\n",
    "\n",
    "#     # Read cached embeddings\n",
    "#     with open(embedding_cache_path, \"rb\") as fIn:\n",
    "#         cache_data = pickle.load(fIn)\n",
    "    \n",
    "#     # Extract corpus text and embeddings from cache pkl  \n",
    "#     cache_corpus_texts = cache_data['text']\n",
    "#     cache_corpus_embeddings = cache_data['embeddings']\n",
    "\n",
    "#     print('##### IDENTIFYING CORPUS TEXTS NOT IN CACHE #####')\n",
    "#     corpus_text_not_in_cache = []\n",
    "#     for i, text in enumerate(current_corpus_texts):\n",
    "#         if text not in cache_corpus_texts:\n",
    "#             print('> TEXT NO. {:,.0f} ({:,.0%} OF TOTAL DATASET)'.format(i, len(corpus_text_not_in_cache)/len(current_corpus_texts)))\n",
    "#             corpus_text_not_in_cache.append(text)\n",
    "    \n",
    "#     if corpus_text_not_in_cache != []:\n",
    "        \n",
    "#         # Apply the cleaning function to the 'corpus_text' column\n",
    "#         print('##### CLEANING NEW CORPUS TEXTS #####')\n",
    "#         corpus_text_not_in_cache = pd.Series(corpus_text_not_in_cache).apply(clean_text).to_list()\n",
    "\n",
    "#         # Encode ONLY the current corpus texts that aren't in cache into embeddings\n",
    "#         print('##### ENCODING IDENTIFIED CORPUS TEXTS #####')\n",
    "#         remaining_corpus_embeddings = model.encode(corpus_text_not_in_cache,show_progress_bar=True,convert_to_tensor=True)\n",
    "#     else:\n",
    "#         print('> NO NEW CORPUS TEXTS')\n",
    "#         remaining_corpus_embeddings = torch.empty(0)\n",
    "        \n",
    "#     # Joining corpus data into single objects for export later\n",
    "#     corpus_embeddings = torch.cat((cache_corpus_embeddings,remaining_corpus_embeddings), dim=0)\n",
    "#     corpus_texts = cache_corpus_texts + corpus_text_not_in_cache\n",
    "\n",
    "else:\n",
    "    print('##### CACHED EMBEDDINGS PICKLE NOT FOUND #####')\n",
    "    corpus_texts_reviews = current_corpus_texts_reviews\n",
    "\n",
    "    # Encode ALL the current corpus texts into embeddings\n",
    "    print('##### ENCODING ALL CORPUS TEXTS #####')\n",
    "    storage_dict = {}\n",
    "    for corpus_name, corpus_text in zip(['embeddings_reviews'],[corpus_texts_reviews]):\n",
    "        print(f'> {corpus_name}')\n",
    "        text_cleaned = corpus_text\n",
    "        corpus_embeddings = model.encode(text_cleaned,show_progress_bar=True,convert_to_tensor=True)\n",
    "        \n",
    "        storage_dict['text_'+corpus_name.split('_')[1]] = corpus_text \n",
    "        storage_dict[corpus_name] = corpus_embeddings\n",
    "        \n",
    "# Update & export complete text and embeddings as pkl for future executions\n",
    "print('##### EXPORTING  #####')\n",
    "with open(embedding_cache_path, \"wb\") as fOut:\n",
    "    pickle.dump(storage_dict, fOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c95ea-5e0e-4d95-8d27-636e10ff21ee",
   "metadata": {},
   "source": [
    "#### Weight Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a419279-cf6e-47ff-8be9-1e08cd08170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight tensor\n",
    "weights = torch.tensor([1])\n",
    "embeddings = ['embeddings_reviews']\n",
    "corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "for i, corpus in enumerate(embeddings):\n",
    "    \n",
    "    # Weight the vectors with the specified weights\n",
    "    weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "    \n",
    "    \n",
    "    # Add the weighted embeddings to the corpus_embeddings\n",
    "    corpus_embeddings += weighted_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31ecc-a88c-4e96-95ef-a343b90eb6f6",
   "metadata": {},
   "source": [
    "#### Encode Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20b05a61-ceba-4477-bb48-806345496e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0de043ec3e81478887fd3f08b2e53e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the query\n",
    "query = \"Cozy cabin close to beach\"\n",
    "clean_query = pd.Series(query).apply(clean_text)\n",
    "query_embedding = model.encode(query,show_progress_bar=True,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aec154-05ce-4ce8-bd61-ff41ccbed3fc",
   "metadata": {},
   "source": [
    "#### Apply Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0b5ed85-b614-423a-bcb3-f10cde096805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['name', 'description', 'corpus_text_host', 'corpus_text_reviews'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m similar_indices \u001b[38;5;241m=\u001b[39m search_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m:top_k]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Extract the actual sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m similar_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msimilar_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_text_host\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_text_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m similar_sentences[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m similar_indices]\n\u001b[0;32m     13\u001b[0m similar_sentences\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:873\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;66;03m# AttributeError for IntervalTree get_value\u001b[39;00m\n\u001b[0;32m    872\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1053\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1003\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[1;32m-> 1003\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1004\u001b[0m     axis: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1006\u001b[0m }\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1004\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1004\u001b[0m     axis: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1006\u001b[0m }\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1254\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 1254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1298\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m   1297\u001b[0m     axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;66;03m# We (temporarily) allow for some missing keys with .loc, except in\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;66;03m# some cases (e.g. setting) in which \"raise_missing\" will be False\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_missing:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['name', 'description', 'corpus_text_host', 'corpus_text_reviews'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "search_results = util.semantic_search(\n",
    "    query_embedding, corpus_embeddings, top_k=top_k\n",
    ")\n",
    "\n",
    "# Extract the indices of the most similar sentences\n",
    "similar_indices = search_results[0][0:top_k]\n",
    "\n",
    "# Extract the actual sentences\n",
    "similar_sentences = df.loc[[item['corpus_id'] for item in similar_indices],['name','description','corpus_text_host','corpus_text_reviews']]\n",
    "similar_sentences['score'] = [item['score'] for item in similar_indices]\n",
    "similar_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b8ae-ea7e-4ccc-acbf-c2af9c922479",
   "metadata": {},
   "source": [
    "### Test optimal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc4af379-5bb0-455d-9b9b-b8fd662ebb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac3f50a2-5d14-41d4-8cd1-56d2cea49d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.00, 1.00] | Max Score: 0.488\n",
      "Weights: [0.10, 0.90] | Max Score: 0.487\n",
      "Weights: [0.20, 0.80] | Max Score: 0.483\n",
      "Weights: [0.30, 0.70] | Max Score: 0.474\n",
      "Weights: [0.40, 0.60] | Max Score: 0.462\n",
      "Weights: [0.50, 0.50] | Max Score: 0.448\n",
      "Weights: [0.60, 0.40] | Max Score: 0.438\n",
      "Weights: [0.70, 0.30] | Max Score: 0.424\n",
      "Weights: [0.80, 0.20] | Max Score: 0.407\n",
      "Weights: [0.90, 0.10] | Max Score: 0.390\n"
     ]
    }
   ],
   "source": [
    "# Create a weight tensor\n",
    "\n",
    "for j in range(10):\n",
    "    \n",
    "    weights = torch.tensor([j/10, 1-(j/10)])\n",
    "    embeddings = ['embeddings_host','embeddings_reviews']\n",
    "    corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "    for i, corpus in enumerate(embeddings):\n",
    "\n",
    "        # Weight the vectors with the specified weights\n",
    "        weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "\n",
    "\n",
    "        # Add the weighted embeddings to the corpus_embeddings\n",
    "        corpus_embeddings += weighted_embeddings\n",
    "        \n",
    "    # Encode the query\n",
    "    query = \"Romantic for couple in mountains\"\n",
    "    clean_query = pd.Series(query).apply(clean_text)\n",
    "    query_embedding = model.encode(query,show_progress_bar=False,convert_to_tensor=True)\n",
    "    search_results = util.semantic_search(\n",
    "        query_embedding, corpus_embeddings, top_k=1\n",
    "    )\n",
    "\n",
    "    # Extract the indices of the most similar sentences\n",
    "    score = search_results[0][0]['score']\n",
    "    \n",
    "    print('Weights: [{:,.2f}, {:,.2f}]'.format(j/10, 1-(j/10)), '| Max Score: {:,.3f}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
