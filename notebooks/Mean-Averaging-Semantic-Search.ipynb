{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4f7f95-e2b2-4b29-8d9c-3b0faa9f51e4",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11d8a18f-b8dc-4662-8541-424d00128d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f304a4-5fc5-4425-a828-4350651bc53e",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "282910ba-7118-4afb-b104-b3310b1b7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Listings data read successfully.\n",
      "Reviews data read successfully.\n",
      "Transforming data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "\n",
    "##\n",
    "## Read\n",
    "##\n",
    "\n",
    "## Listings\n",
    "listings_folder_path = '..\\\\data\\\\raw\\\\listings'\n",
    "\n",
    "# Initialize an empty DataFrame for listings\n",
    "df_listings = pd.DataFrame()\n",
    "\n",
    "# Iterate through each listing file in the folder\n",
    "for listing_file in os.listdir(listings_folder_path):\n",
    "    listing_file_path = os.path.join(listings_folder_path, listing_file)\n",
    "\n",
    "    # Read the CSV file with gzip compression\n",
    "    df = pd.read_csv(listing_file_path, compression='gzip')\n",
    "\n",
    "    # Concatenate the current DataFrame with the overall listings DataFrame\n",
    "    df_listings = pd.concat([df, df_listings])\n",
    "\n",
    "print(\"Listings data read successfully.\")\n",
    "\n",
    "## Reviews\n",
    "reviews_folder_path = '..\\\\data\\\\raw\\\\reviews'\n",
    "\n",
    "# Initialize an empty DataFrame for reviews\n",
    "df_reviews = pd.DataFrame()\n",
    "\n",
    "# Iterate through each review file in the folder\n",
    "for review_file in os.listdir(reviews_folder_path):\n",
    "    review_file_path = os.path.join(reviews_folder_path, review_file)\n",
    "\n",
    "    # Read the CSV file with gzip compression\n",
    "    df = pd.read_csv(review_file_path, compression='gzip')\n",
    "\n",
    "    # Concatenate the current DataFrame with the overall reviews DataFrame\n",
    "    df_reviews = pd.concat([df, df_reviews])\n",
    "\n",
    "print(\"Reviews data read successfully.\")\n",
    "\n",
    "print(\"Transforming data...\")\n",
    "\n",
    "##\n",
    "## Transform\n",
    "##\n",
    "\n",
    "# Listings\n",
    "# Split name into actual name and summary\n",
    "df_listings['subtext'] = df_listings['name'].str.split(' · ').str[1:].apply(lambda x: ' · '.join(x))\n",
    "df_listings['subtext'] = df_listings['subtext'].str.replace('·', '•')\n",
    "df_listings['name'] = df_listings['name'].str.split(' · ').str[0]\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_listings.rename(columns={'listing_url': 'link', 'picture_url': 'photo', 'neighbourhood': 'location','review_scores_rating':'starRating','latitude':'lat','longitude':'long'}, inplace=True)\n",
    "\n",
    "listings_id_column = 'id'\n",
    "listings_nlp_columns = [\n",
    "    'amenities',\n",
    "    'accommodates',\n",
    "    'name',\n",
    "    'subtext',\n",
    "    'property_type',\n",
    "    'room_type',\n",
    "    'location',\n",
    "    'neighbourhood_cleansed',\n",
    "    'description'\n",
    "]\n",
    "\n",
    "# Other columns to save in the DataFrame\n",
    "cols_aux_final = ['id', 'name', 'subtext', 'description', 'link', 'photo', 'price', 'location','starRating','lat','long']\n",
    "\n",
    "# Combine specified columns into a new column 'corpus_text_host'\n",
    "df_listings.loc[:, 'corpus_text_host'] = ''\n",
    "for nlp_col in listings_nlp_columns:\n",
    "    df_listings.loc[:, 'corpus_text_host'] += ' ' + df_listings.loc[:, nlp_col].fillna('').astype(str) + '. '\n",
    "\n",
    "# Select final columns for the listings DataFrame\n",
    "df_listings = df_listings[cols_aux_final + ['corpus_text_host']]\n",
    "\n",
    "# Rename the 'comments' column to 'corpus_text_reviews'\n",
    "df_reviews.rename(columns={'comments': 'corpus_text_reviews'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "661cdd69-3f8d-429f-bfdb-1366a5151857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.loc[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6f478-6375-4327-be22-bfb541e34684",
   "metadata": {},
   "source": [
    "### Preprocess for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ad83fb12-2a98-419a-b002-2d18d940a71f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to perform all cleaning steps\n",
    "def clean_text(text):\n",
    "        \n",
    "    # Remove punctuation\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = re.split(r'\\W+', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [porter_stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Set of English stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6ffbef05-abf7-4075-929e-a402df2f2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['corpus_text_reviews'] = df_reviews['corpus_text_reviews'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "439072f0-7d07-42db-b5b2-85d7deb2bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews[df_reviews['corpus_text_reviews'].str.len()>1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "64048e14-1b91-43da-81ca-f0df75acc158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>corpus_text_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8941071</td>\n",
       "      <td>68391055</td>\n",
       "      <td>2016-04-04</td>\n",
       "      <td>10164333</td>\n",
       "      <td>Smruti</td>\n",
       "      <td>[daniel, great, host, extrem, respons, question, flexibl, extend, stay, kind, apart, exactli, look, pictur, cute, clean, comfort, locat, amaz, quiet, block, close, restaur, whole, food, stay, 2, month, start, job, realli, felt, like, home, thank, daniel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8941071</td>\n",
       "      <td>153719836</td>\n",
       "      <td>2017-05-21</td>\n",
       "      <td>97944097</td>\n",
       "      <td>Rob</td>\n",
       "      <td>[apart, great, u, spend, weekend, weho, clean, quiet, night, street, away, good, restaur, cafe, bar, u, thing, miss, iron, cloth, underground, park, good, bmw, x5, big, space, would, fit, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8941071</td>\n",
       "      <td>147589354</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>4123723</td>\n",
       "      <td>Widya</td>\n",
       "      <td>[daniel, great, host, concern, guest, wellb, easi, reach, fast, repli, apart, perfectli, locat, walk, access, suppli, shop, hip, place, jog, neighborhood, cant, better, apart, spaciou, bright, comfort, thing, miss, like, toaster, bit, darkli, lit, night, definit, great, valu, moneybrthank, daniel]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8941071</td>\n",
       "      <td>145742425</td>\n",
       "      <td>2017-04-19</td>\n",
       "      <td>1459499</td>\n",
       "      <td>Darian</td>\n",
       "      <td>[great, locat, spaciou, daniel, place, home, away, home, also, flexibl, check, inout, time, realli, appreci]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8941071</td>\n",
       "      <td>144400833</td>\n",
       "      <td>2017-04-15</td>\n",
       "      <td>98494277</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>[daniel, place, expect, realli, good, locat, calm, cosybrdaniel, help, make, stress, free, experi, brwould, recommend, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>9260889</td>\n",
       "      <td>57583165</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>1965583</td>\n",
       "      <td>Iqbal</td>\n",
       "      <td>[home, describ, tast, decor, famili, friendli, quiet, neighborhood, clean, well, furnish, kitchen, well, stock, pan, dinnerwar, flatwar, backyard, nice, outdoor, din, home, close, univers, hollywood, blvd, sara, hous, manag, quick, respond, queri, easili, reachabl, text, phone, even, stock, kitchen, basic, perish, nice, gestur, kid, enjoy, home, much, comfort, well, entertain, thank, vast, collect, toy, book, dvd, roku, top, well, decor, christma, porch, light, christma, tree, thank, sara, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>8987171</td>\n",
       "      <td>263744451</td>\n",
       "      <td>2018-05-12</td>\n",
       "      <td>78791844</td>\n",
       "      <td>Jhj931220@Nate.Com</td>\n",
       "      <td>[1파머스마켓, 그로브몰라끄마, 큰, 마켓, 가까워서, 걸어다니며, 구경하기, 좋았음, br2숙소의, 안전이, 보장된, 곳, 아파트, 단지, 내에, 거리도, 깨끗하고, 경비가, 철저함br3방에서, 보이는, 뷰가, 멋짐, 매일, 아침, 뷰를, 보며, 일어날때, 설렘, br4우리가, 영어를, 못하여, 많은, 대화가없었지만, 배려를, 해주시는게, 느껴졌음, br5방과, 욕실은, 매우깨끗했고, 침대가, 편했음, br6우버를, 이용하여, 다녔지만, 모든, 관광지를, 다니기에, 적절한, 위치였다]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>8987171</td>\n",
       "      <td>253214879</td>\n",
       "      <td>2018-04-13</td>\n",
       "      <td>99804734</td>\n",
       "      <td>Johnny</td>\n",
       "      <td>[小区很新, 房主也很友善, 女主人是一位很友善会讲中文的漂亮小姐姐, 男主人整天也是对房客笑呵呵的, 唯一的问题是对于自驾的朋友, 因为小区停车位很少, 晚归的话停车不是很方便, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>8987171</td>\n",
       "      <td>240928049</td>\n",
       "      <td>2018-03-06</td>\n",
       "      <td>174145199</td>\n",
       "      <td>Heuidong</td>\n",
       "      <td>[큰, 단지, 안에, 있는, 아파트입니다, 치안이, 좋고, 근처에, 쇼핑몰도, 있습니다, 비버리힐즈, 할리우드와, 가까우며, 방에서, 보는, 뷰가, 굉장히, 좋습니다, 호스트는, 굉장히, 친절하며, 편안함을, 줍니다, 완전, 강추]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>8987171</td>\n",
       "      <td>232875972</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>57720646</td>\n",
       "      <td>채린</td>\n",
       "      <td>[호스트, 두분다, 친절하셨고, 집도, 깨끗했다, 그리고, 뷰도, 좋고, 그로브몰이랑도, 가까워서, 위치도, 좋고, 치안도, 좋았다, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     listing_id         id        date  reviewer_id       reviewer_name  \\\n",
       "0       8941071   68391055  2016-04-04     10164333              Smruti   \n",
       "1       8941071  153719836  2017-05-21     97944097                 Rob   \n",
       "2       8941071  147589354  2017-04-27      4123723               Widya   \n",
       "3       8941071  145742425  2017-04-19      1459499              Darian   \n",
       "4       8941071  144400833  2017-04-15     98494277             Charlie   \n",
       "..          ...        ...         ...          ...                 ...   \n",
       "489     9260889   57583165  2015-12-27      1965583               Iqbal   \n",
       "490     8987171  263744451  2018-05-12     78791844  Jhj931220@Nate.Com   \n",
       "491     8987171  253214879  2018-04-13     99804734              Johnny   \n",
       "492     8987171  240928049  2018-03-06    174145199            Heuidong   \n",
       "493     8987171  232875972  2018-02-06     57720646                  채린   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     corpus_text_reviews  \n",
       "0                                                                                                                                                                                                                                                         [daniel, great, host, extrem, respons, question, flexibl, extend, stay, kind, apart, exactli, look, pictur, cute, clean, comfort, locat, amaz, quiet, block, close, restaur, whole, food, stay, 2, month, start, job, realli, felt, like, home, thank, daniel]  \n",
       "1                                                                                                                                                                                                                                                                                                                          [apart, great, u, spend, weekend, weho, clean, quiet, night, street, away, good, restaur, cafe, bar, u, thing, miss, iron, cloth, underground, park, good, bmw, x5, big, space, would, fit, ]  \n",
       "2                                                                                                                                                                                                             [daniel, great, host, concern, guest, wellb, easi, reach, fast, repli, apart, perfectli, locat, walk, access, suppli, shop, hip, place, jog, neighborhood, cant, better, apart, spaciou, bright, comfort, thing, miss, like, toaster, bit, darkli, lit, night, definit, great, valu, moneybrthank, daniel]  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                           [great, locat, spaciou, daniel, place, home, away, home, also, flexibl, check, inout, time, realli, appreci]  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                               [daniel, place, expect, realli, good, locat, calm, cosybrdaniel, help, make, stress, free, experi, brwould, recommend, ]  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...  \n",
       "489  [home, describ, tast, decor, famili, friendli, quiet, neighborhood, clean, well, furnish, kitchen, well, stock, pan, dinnerwar, flatwar, backyard, nice, outdoor, din, home, close, univers, hollywood, blvd, sara, hous, manag, quick, respond, queri, easili, reachabl, text, phone, even, stock, kitchen, basic, perish, nice, gestur, kid, enjoy, home, much, comfort, well, entertain, thank, vast, collect, toy, book, dvd, roku, top, well, decor, christma, porch, light, christma, tree, thank, sara, l...  \n",
       "490                                                                                                                                                                                                                                   [1파머스마켓, 그로브몰라끄마, 큰, 마켓, 가까워서, 걸어다니며, 구경하기, 좋았음, br2숙소의, 안전이, 보장된, 곳, 아파트, 단지, 내에, 거리도, 깨끗하고, 경비가, 철저함br3방에서, 보이는, 뷰가, 멋짐, 매일, 아침, 뷰를, 보며, 일어날때, 설렘, br4우리가, 영어를, 못하여, 많은, 대화가없었지만, 배려를, 해주시는게, 느껴졌음, br5방과, 욕실은, 매우깨끗했고, 침대가, 편했음, br6우버를, 이용하여, 다녔지만, 모든, 관광지를, 다니기에, 적절한, 위치였다]  \n",
       "491                                                                                                                                                                                                                                                                                                                                                                                                                         [小区很新, 房主也很友善, 女主人是一位很友善会讲中文的漂亮小姐姐, 男主人整天也是对房客笑呵呵的, 唯一的问题是对于自驾的朋友, 因为小区停车位很少, 晚归的话停车不是很方便, ]  \n",
       "492                                                                                                                                                                                                                                                                                                                                                                                    [큰, 단지, 안에, 있는, 아파트입니다, 치안이, 좋고, 근처에, 쇼핑몰도, 있습니다, 비버리힐즈, 할리우드와, 가까우며, 방에서, 보는, 뷰가, 굉장히, 좋습니다, 호스트는, 굉장히, 친절하며, 편안함을, 줍니다, 완전, 강추]  \n",
       "493                                                                                                                                                                                                                                                                                                                                                                                                                                         [호스트, 두분다, 친절하셨고, 집도, 깨끗했다, 그리고, 뷰도, 좋고, 그로브몰이랑도, 가까워서, 위치도, 좋고, 치안도, 좋았다, ]  \n",
       "\n",
       "[494 rows x 6 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574c0b2-6638-46f9-a6f3-ef86679e4944",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "734fe9cf-0c54-4b0d-a701-2c118b14084a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### INITIALIZING SBERT MODEL #####\n",
      "##### ENCODING ALL CORPUS TEXTS #####\n",
      "> embeddings_reviews\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeee4fe164b4ad58b94f417ec6b3b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### EXPORTING  #####\n"
     ]
    }
   ],
   "source": [
    "# SBERT model name\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "\n",
    "# Initialize SBERT model\n",
    "print('##### INITIALIZING SBERT MODEL #####')\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Cached Embeddings Path (changes according to model)\n",
    "embedding_cache_path = f'cache\\\\cached-embeddings-{model_name}_mean_average_clean.pkl'\n",
    "\n",
    "# Current corpus texts\n",
    "current_corpus_texts_reviews = df_reviews['corpus_text_reviews']\n",
    "\n",
    "# Encode ALL the current corpus texts into embeddings\n",
    "print('##### ENCODING ALL CORPUS TEXTS #####')\n",
    "storage_dict = {}\n",
    "for corpus_name, corpus_text in zip(['embeddings_reviews'],[current_corpus_texts_reviews]):\n",
    "    print(f'> {corpus_name}')\n",
    "    corpus_embeddings = model.encode(corpus_text,show_progress_bar=True,convert_to_tensor=True)\n",
    "\n",
    "    storage_dict['text_'+corpus_name.split('_')[1]] = corpus_text \n",
    "    storage_dict[corpus_name] = corpus_embeddings\n",
    "    \n",
    "    for col in ['listing_id']:\n",
    "        storage_dict[col] = df_reviews[col].to_list()\n",
    "    \n",
    "        \n",
    "# Update & export complete text and embeddings as pkl for future executions\n",
    "print('##### EXPORTING  #####')\n",
    "with open(embedding_cache_path, \"wb\") as fOut:\n",
    "    pickle.dump(storage_dict, fOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c95ea-5e0e-4d95-8d27-636e10ff21ee",
   "metadata": {},
   "source": [
    "#### Weight Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5b26c07d-9010-4354-9ee5-70ee455f19ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(storage_dict)\n",
    "df['embeddings_reviews'] = df['embeddings_reviews'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cb43bb58-09c6-492f-b46b-aa5031ffd57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod(x):\n",
    "    print(x)\n",
    "    return torch.prod(torch.tensor(x),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f222340a-8839-40a6-ace4-a21767ea1fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30    [tensor(-0.0024), tensor(0.0206), tensor(0.0077), tensor(0.0575), tensor(-0.1284), tensor(0.0862), tensor(0.0500), tensor(-0.0066), tensor(-0.0383), tensor(-0.0341), tensor(0.0182), tensor(-0.0815), tensor(0.0348), tensor(-0.0187), tensor(-0.0028), tensor(0.0786), tensor(0.1157), tensor(-0.0105), tensor(-0.0589), tensor(0.0636), tensor(-0.0822), tensor(-0.0003), tensor(0.0335), tensor(0.0440), tensor(-0.0271), tensor(-0.0251), tensor(0.0467), tensor(0.0090), tensor(0.0279), tensor(0.0114), t...\n",
       "31    [tensor(0.0765), tensor(0.0222), tensor(-0.0268), tensor(0.0626), tensor(-0.0333), tensor(-0.0570), tensor(-0.0566), tensor(-0.0922), tensor(0.0163), tensor(-0.0328), tensor(0.0463), tensor(-0.0725), tensor(-0.0354), tensor(0.0587), tensor(0.0654), tensor(-0.1209), tensor(0.0185), tensor(-0.0587), tensor(-0.0003), tensor(-0.0067), tensor(-0.0058), tensor(-0.0511), tensor(-0.0405), tensor(0.0499), tensor(-0.0481), tensor(-0.0501), tensor(-0.0258), tensor(0.0455), tensor(-0.0692), tensor(-0.03...\n",
       "Name: embeddings_reviews, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[30,31],'embeddings_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d317407c-e229-47a8-a779-4c68f1c4ded0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [167]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mprod(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not determine the shape of object type 'Series'"
     ]
    }
   ],
   "source": [
    "torch.prod(torch.tensor(df.loc[[30,31],'embeddings_reviews']),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3d6e10c3-6b7c-4c93-938d-4e519d2577c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30    [tensor(-0.0024), tensor(0.0206), tensor(0.0077), tensor(0.0575), tensor(-0.1284), tensor(0.0862), tensor(0.0500), tensor(-0.0066), tensor(-0.0383), tensor(-0.0341), tensor(0.0182), tensor(-0.0815), tensor(0.0348), tensor(-0.0187), tensor(-0.0028), tensor(0.0786), tensor(0.1157), tensor(-0.0105), tensor(-0.0589), tensor(0.0636), tensor(-0.0822), tensor(-0.0003), tensor(0.0335), tensor(0.0440), tensor(-0.0271), tensor(-0.0251), tensor(0.0467), tensor(0.0090), tensor(0.0279), tensor(0.0114), t...\n",
      "Name: 8925296, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [161]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlisting_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membeddings_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\groupby\\generic.py:226\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[0;32m    221\u001b[0m     _apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m, examples\u001b[38;5;241m=\u001b[39m_apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    223\u001b[0m     )\n\u001b[0;32m    224\u001b[0m )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:859\u001b[0m, in \u001b[0;36m_GroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 859\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m    869\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _group_selection_context(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:892\u001b[0m, in \u001b[0;36m_GroupBy._python_apply_general\u001b[1;34m(self, f, data)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, f: F, data: FrameOrSeriesUnion\n\u001b[0;32m    876\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     keys, values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_applied_output(\n\u001b[0;32m    895\u001b[0m         keys, values, not_indexed_same\u001b[38;5;241m=\u001b[39mmutated \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated\n\u001b[0;32m    896\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:220\u001b[0m, in \u001b[0;36mBaseGrouper.apply\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    219\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 220\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes):\n\u001b[0;32m    222\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Input \u001b[1;32mIn [160]\u001b[0m, in \u001b[0;36mprod\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(x):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprod(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: could not determine the shape of object type 'Series'"
     ]
    }
   ],
   "source": [
    "df.groupby(['listing_id'])['embeddings_reviews'].apply(prod).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd2d7e-b42a-489c-8872-7a32ff127a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e89b33d-60fe-4ded-98c0-0267afc04864",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mean() received an invalid combination of arguments - got (axis=int, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m embeddings_for_listing \u001b[38;5;241m=\u001b[39m storage_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_reviews\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate mean of embeddings for the current listing\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m mean_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_for_listing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Store mean embeddings in the dictionary\u001b[39;00m\n\u001b[0;32m     15\u001b[0m mean_embeddings_by_listing[listing_id] \u001b[38;5;241m=\u001b[39m mean_embeddings\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3370\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   3368\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   3369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3370\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3373\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: mean() received an invalid combination of arguments - got (axis=int, dtype=NoneType, out=NoneType, ), but expected one of:\n * (*, torch.dtype dtype)\n * (tuple of ints dim, bool keepdim, *, torch.dtype dtype)\n * (tuple of names dim, bool keepdim, *, torch.dtype dtype)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store mean averaged embeddings for each listing\n",
    "mean_embeddings_by_listing = {}\n",
    "\n",
    "# Iterate over each listing\n",
    "for idx, listing_id in enumerate(storage_dict['listing_id']):\n",
    "    # Extract embeddings for the current listing\n",
    "    embeddings_for_listing = storage_dict['embeddings_reviews'][idx]\n",
    "\n",
    "    # Calculate mean of embeddings for the current listing\n",
    "    mean_embeddings = np.mean(embeddings_for_listing, axis=0)\n",
    "\n",
    "    # Store mean embeddings in the dictionary\n",
    "    mean_embeddings_by_listing[listing_id] = mean_embeddings\n",
    "\n",
    "# Now, mean_embeddings_by_listing contains the mean embeddings for each listing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e173d-4c7e-492e-bb63-192269c5fc90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a419279-cf6e-47ff-8be9-1e08cd08170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight tensor\n",
    "weights = torch.tensor([1])\n",
    "embeddings = ['embeddings_reviews']\n",
    "corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "for i, corpus in enumerate(embeddings):\n",
    "    \n",
    "    # Weight the vectors with the specified weights\n",
    "    weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "    \n",
    "    # Add the weighted embeddings to the corpus_embeddings\n",
    "    corpus_embeddings += weighted_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "227d9423-b647-4f9a-b045-d2be28905a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0187,  0.0276,  0.0005,  ..., -0.0364,  0.0141, -0.0065],\n",
       "        [ 0.0339, -0.0022, -0.0277,  ..., -0.0818,  0.0386,  0.0328],\n",
       "        [ 0.0187,  0.0276,  0.0005,  ..., -0.0364,  0.0141, -0.0065],\n",
       "        ...,\n",
       "        [ 0.0078,  0.0219, -0.0255,  ..., -0.0012, -0.0163, -0.0233],\n",
       "        [-0.0458,  0.0403,  0.0209,  ...,  0.0179,  0.0692, -0.0112],\n",
       "        [-0.0419, -0.0097, -0.0258,  ..., -0.0459,  0.0498,  0.0442]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b31ecc-a88c-4e96-95ef-a343b90eb6f6",
   "metadata": {},
   "source": [
    "#### Encode Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b05a61-ceba-4477-bb48-806345496e08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cd8fc968da4980be8693036ba0f5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the query\n",
    "query = \"Cozy cabin close to beach\"\n",
    "clean_query = pd.Series(query).apply(clean_text)\n",
    "query_embedding = model.encode(query,show_progress_bar=True,convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aec154-05ce-4ce8-bd61-ff41ccbed3fc",
   "metadata": {},
   "source": [
    "#### Apply Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0b5ed85-b614-423a-bcb3-f10cde096805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['name', 'description', 'corpus_text_host', 'corpus_text_reviews'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m similar_indices \u001b[38;5;241m=\u001b[39m search_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m:top_k]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Extract the actual sentences\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m similar_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msimilar_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_text_host\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcorpus_text_reviews\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m similar_sentences[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m similar_indices]\n\u001b[0;32m     13\u001b[0m similar_sentences\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:873\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;66;03m# AttributeError for IntervalTree get_value\u001b[39;00m\n\u001b[0;32m    872\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1053\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# ugly hack for GH #836\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m-> 1053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_multi_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1003\u001b[0m, in \u001b[0;36m_LocIndexer._multi_take\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[1;32m-> 1003\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1004\u001b[0m     axis: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1006\u001b[0m }\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1004\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03mCreate the indexers for the passed tuple of keys, and\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03mexecutes the take operation. This allows the take operation to be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03mvalues: same type as the object being indexed\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# GH 836\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 1004\u001b[0m     axis: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1005\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (key, axis) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_AXIS_ORDERS)\n\u001b[0;32m   1006\u001b[0m }\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(d, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1254\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1252\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 1254\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\users\\anton\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1298\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m   1297\u001b[0m     axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;66;03m# We (temporarily) allow for some missing keys with .loc, except in\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;66;03m# some cases (e.g. setting) in which \"raise_missing\" will be False\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_missing:\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['name', 'description', 'corpus_text_host', 'corpus_text_reviews'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "\n",
    "search_results = util.semantic_search(\n",
    "    query_embedding, corpus_embeddings, top_k=top_k\n",
    ")\n",
    "\n",
    "# Extract the indices of the most similar sentences\n",
    "similar_indices = search_results[0][0:top_k]\n",
    "\n",
    "# Extract the actual sentences\n",
    "similar_sentences = df.loc[[item['corpus_id'] for item in similar_indices],['name','description','corpus_text_host','corpus_text_reviews']]\n",
    "similar_sentences['score'] = [item['score'] for item in similar_indices]\n",
    "similar_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a301b8ae-ea7e-4ccc-acbf-c2af9c922479",
   "metadata": {},
   "source": [
    "### Test optimal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc4af379-5bb0-455d-9b9b-b8fd662ebb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ac3f50a2-5d14-41d4-8cd1-56d2cea49d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.00, 1.00] | Max Score: 0.488\n",
      "Weights: [0.10, 0.90] | Max Score: 0.487\n",
      "Weights: [0.20, 0.80] | Max Score: 0.483\n",
      "Weights: [0.30, 0.70] | Max Score: 0.474\n",
      "Weights: [0.40, 0.60] | Max Score: 0.462\n",
      "Weights: [0.50, 0.50] | Max Score: 0.448\n",
      "Weights: [0.60, 0.40] | Max Score: 0.438\n",
      "Weights: [0.70, 0.30] | Max Score: 0.424\n",
      "Weights: [0.80, 0.20] | Max Score: 0.407\n",
      "Weights: [0.90, 0.10] | Max Score: 0.390\n"
     ]
    }
   ],
   "source": [
    "# Create a weight tensor\n",
    "\n",
    "for j in range(10):\n",
    "    \n",
    "    weights = torch.tensor([j/10, 1-(j/10)])\n",
    "    embeddings = ['embeddings_host','embeddings_reviews']\n",
    "    corpus_embeddings = torch.zeros_like(storage_dict[embeddings[0]])  # Initialize an empty tensor\n",
    "\n",
    "    for i, corpus in enumerate(embeddings):\n",
    "\n",
    "        # Weight the vectors with the specified weights\n",
    "        weighted_embeddings = storage_dict[corpus] * weights[i]\n",
    "\n",
    "\n",
    "        # Add the weighted embeddings to the corpus_embeddings\n",
    "        corpus_embeddings += weighted_embeddings\n",
    "        \n",
    "    # Encode the query\n",
    "    query = \"Romantic for couple in mountains\"\n",
    "    clean_query = pd.Series(query).apply(clean_text)\n",
    "    query_embedding = model.encode(query,show_progress_bar=False,convert_to_tensor=True)\n",
    "    search_results = util.semantic_search(\n",
    "        query_embedding, corpus_embeddings, top_k=1\n",
    "    )\n",
    "\n",
    "    # Extract the indices of the most similar sentences\n",
    "    score = search_results[0][0]['score']\n",
    "    \n",
    "    print('Weights: [{:,.2f}, {:,.2f}]'.format(j/10, 1-(j/10)), '| Max Score: {:,.3f}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
